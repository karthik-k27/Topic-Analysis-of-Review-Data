{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DESCRIPTION\n",
    "\n",
    "### Help a leading mobile brand understand the voice of the customer by analyzing the reviews of their product on Amazon and the topics that customers are talking about. You will perform topic modeling on specific parts of speech. Youâ€™ll finally interpret the emerging topics.\n",
    "\n",
    "# Problem Statement: \n",
    "\n",
    "###  A popular mobile phone brand, Lenovo has launched their budget smartphone in the Indian market. The client wants to understand the VOC (voice of the customer) on the product. This will be useful to not just evaluate the current product, but to also get some direction for developing the product pipeline. The client is particularly interested in the different aspects that customers care about. Product reviews by customers on a leading e-commerce site should provide a good view."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Good but need updates and improvements</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Worst mobile i have bought ever, Battery is dr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>when I will get my 10% cash back.... its alrea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>The worst phone everThey have changed the last...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                                             review\n",
       "0          1             Good but need updates and improvements\n",
       "1          0  Worst mobile i have bought ever, Battery is dr...\n",
       "2          1  when I will get my 10% cash back.... its alrea...\n",
       "3          1                                               Good\n",
       "4          0  The worst phone everThey have changed the last..."
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# importing datasets\n",
    "ds = pd.read_csv(\"K8 Reviews v0.2.csv\")\n",
    "ds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14675, 2)"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into training and testing\n",
    "from sklearn.model_selection import train_test_split\n",
    "ds_train,ds_test=train_test_split(ds,test_size=0.2,random_state=45)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Good but need updates and improvements',\n",
       " \"Worst mobile i have bought ever, Battery is draining like hell, backup is only 6 to 7 hours with internet uses, even if I put mobile idle its getting discharged.This is biggest lie from Amazon & Lenove which is not at all expected, they are making full by saying that battery is 4000MAH & booster charger is fake, it takes at least 4 to 5 hours to be fully charged.Don't know how Lenovo will survive by making full of us.Please don;t go for this else you will regret like me.\",\n",
       " 'when I will get my 10% cash back.... its already 15 January..',\n",
       " 'Good',\n",
       " 'The worst phone everThey have changed the last phone but the problem is still same and the amazon is not returning the phone .Highly disappointing of amazon']"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Normalizing the review texts\n",
    "reviews = list(ds.iloc[:,1])\n",
    "sentiments = list(ds.iloc[:,0])\n",
    "reviews[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n#Punctuation removal\\nimport re, string\\nnopunc_reviews=[]\\nfor i in reviews:\\n    result = re.sub('[%s]' % re.escape(string.punctuation),' ', i)\\n    nopunc_reviews.append(result)\\n\\n    \\nprint(len(nopunc_reviews))\\nprint(nopunc_reviews[0:4])\\nprint(result[0:4])\\n\""
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "#Punctuation removal\n",
    "import re, string\n",
    "nopunc_reviews=[]\n",
    "for i in reviews:\n",
    "    result = re.sub('[%s]' % re.escape(string.punctuation),' ', i)\n",
    "    nopunc_reviews.append(result)\n",
    "\n",
    "    \n",
    "print(len(nopunc_reviews))\n",
    "print(nopunc_reviews[0:4])\n",
    "print(result[0:4])\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Good', 'but', 'need', 'updates', 'and', 'improvements']"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Tokenization\n",
    "from nltk.tokenize import word_tokenize\n",
    "token1 = word_tokenize(reviews[0])\n",
    "token1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Good', 'but', 'need', 'updates', 'and', 'improvements'], ['Worst', 'mobile', 'i', 'have', 'bought', 'ever', ',', 'Battery', 'is', 'draining', 'like', 'hell', ',', 'backup', 'is', 'only', '6', 'to', '7', 'hours', 'with', 'internet', 'uses', ',', 'even', 'if', 'I', 'put', 'mobile', 'idle', 'its', 'getting', 'discharged.This', 'is', 'biggest', 'lie', 'from', 'Amazon', '&', 'Lenove', 'which', 'is', 'not', 'at', 'all', 'expected', ',', 'they', 'are', 'making', 'full', 'by', 'saying', 'that', 'battery', 'is', '4000MAH', '&', 'booster', 'charger', 'is', 'fake', ',', 'it', 'takes', 'at', 'least', '4', 'to', '5', 'hours', 'to', 'be', 'fully', 'charged.Do', \"n't\", 'know', 'how', 'Lenovo', 'will', 'survive', 'by', 'making', 'full', 'of', 'us.Please', 'don', ';', 't', 'go', 'for', 'this', 'else', 'you', 'will', 'regret', 'like', 'me', '.'], ['when', 'I', 'will', 'get', 'my', '10', '%', 'cash', 'back', '....', 'its', 'already', '15', 'January', '..'], ['Good']]\n"
     ]
    }
   ],
   "source": [
    "tokens=[]\n",
    "\n",
    "for token in reviews:\n",
    "    tokens.append(word_tokenize(token))\n",
    "\n",
    "print(tokens[0:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('Good', 'JJ'), ('but', 'CC'), ('need', 'VBP'), ('updates', 'NNS'), ('and', 'CC'), ('improvements', 'NNS')], [('Worst', 'NNP'), ('mobile', 'NN'), ('i', 'NN'), ('have', 'VBP'), ('bought', 'VBN'), ('ever', 'RB'), (',', ','), ('Battery', 'NNP'), ('is', 'VBZ'), ('draining', 'VBG'), ('like', 'IN'), ('hell', 'NN'), (',', ','), ('backup', 'NN'), ('is', 'VBZ'), ('only', 'RB'), ('6', 'CD'), ('to', 'TO'), ('7', 'CD'), ('hours', 'NNS'), ('with', 'IN'), ('internet', 'JJ'), ('uses', 'NNS'), (',', ','), ('even', 'RB'), ('if', 'IN'), ('I', 'PRP'), ('put', 'VBP'), ('mobile', 'JJ'), ('idle', 'NN'), ('its', 'PRP$'), ('getting', 'VBG'), ('discharged.This', 'NN'), ('is', 'VBZ'), ('biggest', 'JJS'), ('lie', 'NN'), ('from', 'IN'), ('Amazon', 'NNP'), ('&', 'CC'), ('Lenove', 'NNP'), ('which', 'WDT'), ('is', 'VBZ'), ('not', 'RB'), ('at', 'IN'), ('all', 'DT'), ('expected', 'VBN'), (',', ','), ('they', 'PRP'), ('are', 'VBP'), ('making', 'VBG'), ('full', 'JJ'), ('by', 'IN'), ('saying', 'VBG'), ('that', 'DT'), ('battery', 'NN'), ('is', 'VBZ'), ('4000MAH', 'CD'), ('&', 'CC'), ('booster', 'JJR'), ('charger', 'NN'), ('is', 'VBZ'), ('fake', 'JJ'), (',', ','), ('it', 'PRP'), ('takes', 'VBZ'), ('at', 'IN'), ('least', 'JJS'), ('4', 'CD'), ('to', 'TO'), ('5', 'CD'), ('hours', 'NNS'), ('to', 'TO'), ('be', 'VB'), ('fully', 'RB'), ('charged.Do', 'VBP'), (\"n't\", 'RB'), ('know', 'VB'), ('how', 'WRB'), ('Lenovo', 'NNP'), ('will', 'MD'), ('survive', 'VB'), ('by', 'IN'), ('making', 'VBG'), ('full', 'JJ'), ('of', 'IN'), ('us.Please', 'JJ'), ('don', 'NN'), (';', ':'), ('t', 'CC'), ('go', 'VB'), ('for', 'IN'), ('this', 'DT'), ('else', 'JJ'), ('you', 'PRP'), ('will', 'MD'), ('regret', 'VB'), ('like', 'IN'), ('me', 'PRP'), ('.', '.')], [('when', 'WRB'), ('I', 'PRP'), ('will', 'MD'), ('get', 'VB'), ('my', 'PRP$'), ('10', 'CD'), ('%', 'NN'), ('cash', 'NN'), ('back', 'RB'), ('....', 'VBZ'), ('its', 'PRP$'), ('already', 'RB'), ('15', 'CD'), ('January', 'NNP'), ('..', 'NN')], [('Good', 'JJ')]]\n"
     ]
    }
   ],
   "source": [
    "#POS tagging\n",
    "pos_tokens=[]\n",
    "\n",
    "for tag in tokens:\n",
    "    pos_tokens.append(nltk.pos_tag(tag))\n",
    "\n",
    "print(pos_tokens[0:4])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "updates NNS\n",
      "improvements NNS\n"
     ]
    }
   ],
   "source": [
    "for word,pos in pos_tokens[0]:\n",
    "    if (pos == 'NN' or pos == 'NNP' or pos == 'NNS' or pos == 'NNPS'):\n",
    "        print(word,pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Worst', 'mobile', 'i', 'have', 'bought', 'ever', ',', 'Battery', 'is', 'draining', 'like', 'hell', ',', 'backup', 'is', 'only', '6', 'to', '7', 'hours', 'with', 'internet', 'uses', ',', 'even', 'if', 'I', 'put', 'mobile', 'idle', 'its', 'getting', 'discharged.This', 'is', 'biggest', 'lie', 'from', 'Amazon', '&', 'Lenove', 'which', 'is', 'not', 'at', 'all', 'expected', ',', 'they', 'are', 'making', 'full', 'by', 'saying', 'that', 'battery', 'is', '4000MAH', '&', 'booster', 'charger', 'is', 'fake', ',', 'it', 'takes', 'at', 'least', '4', 'to', '5', 'hours', 'to', 'be', 'fully', 'charged.Do', \"n't\", 'know', 'how', 'Lenovo', 'will', 'survive', 'by', 'making', 'full', 'of', 'us.Please', 'don', ';', 't', 'go', 'for', 'this', 'else', 'you', 'will', 'regret', 'like', 'me', '.']\n"
     ]
    }
   ],
   "source": [
    "test=[]\n",
    "for word,pos in pos_tokens[1]:\n",
    "    test.append(word)\n",
    "\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good JJ\n",
      "but CC\n",
      "need VBP\n",
      "updates NNS\n",
      "and CC\n",
      "improvements NNS\n"
     ]
    }
   ],
   "source": [
    "for word,pos in pos_tokens[0]:\n",
    "    print(word,pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['updates', 'improvements', 'Worst', 'mobile']"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Extracting Nouns\n",
    "nouns =[]\n",
    "noun_tokens=[]\n",
    "\n",
    "index=[]\n",
    "temp =0\n",
    "count=0\n",
    "i=0\n",
    "while i<len(pos_tokens):\n",
    "    \n",
    "    for word,pos in pos_tokens[i]:\n",
    "    \n",
    "        if (pos == 'NN' or pos == 'NNP' or pos == 'NNS' or pos == 'NNPS'):\n",
    "            nouns.append(word)\n",
    "            count+=1\n",
    "    \n",
    "    noun_tokens.append(nouns[temp:count])\n",
    "    temp=count\n",
    "    \n",
    "    i+=1\n",
    "    \n",
    "\n",
    "      \n",
    "nouns[0:4] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14675\n",
      "['updates', 'improvements', 'Worst', 'mobile', 'i', 'Battery', 'hell', 'backup', 'hours', 'uses', 'idle', 'discharged.This', 'lie', 'Amazon', 'Lenove', 'battery', 'charger', 'hours', 'Lenovo', 'don']\n",
      "['Good but need updates and improvements', \"Worst mobile i have bought ever, Battery is draining like hell, backup is only 6 to 7 hours with internet uses, even if I put mobile idle its getting discharged.This is biggest lie from Amazon & Lenove which is not at all expected, they are making full by saying that battery is 4000MAH & booster charger is fake, it takes at least 4 to 5 hours to be fully charged.Don't know how Lenovo will survive by making full of us.Please don;t go for this else you will regret like me.\", 'when I will get my 10% cash back.... its already 15 January..', 'Good', 'The worst phone everThey have changed the last phone but the problem is still same and the amazon is not returning the phone .Highly disappointing of amazon']\n"
     ]
    }
   ],
   "source": [
    "print(len(pos_tokens))\n",
    "\n",
    "print(nouns[0:20])\n",
    "\n",
    "print(reviews[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14675 14675\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['updates', 'improvements'],\n",
       " ['Worst',\n",
       "  'mobile',\n",
       "  'i',\n",
       "  'Battery',\n",
       "  'hell',\n",
       "  'backup',\n",
       "  'hours',\n",
       "  'uses',\n",
       "  'idle',\n",
       "  'discharged.This',\n",
       "  'lie',\n",
       "  'Amazon',\n",
       "  'Lenove',\n",
       "  'battery',\n",
       "  'charger',\n",
       "  'hours',\n",
       "  'Lenovo',\n",
       "  'don'],\n",
       " ['%', 'cash', 'January', '..']]"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(noun_tokens), len(pos_tokens))\n",
    "noun_tokens[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Good', 'JJ'),\n",
       " ('but', 'CC'),\n",
       " ('need', 'VBP'),\n",
       " ('updates', 'NNS'),\n",
       " ('and', 'CC'),\n",
       " ('improvements', 'NNS')]"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_tokens[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99571\n",
      "['updat', 'improv', 'worst', 'mobil', 'i', 'batteri', 'hell', 'backup', 'hour', 'us', 'idl', 'discharged.thi', 'lie', 'amazon', 'lenov', 'batteri', 'charger', 'hour', 'lenovo', 'don']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "lemmatizer=WordNetLemmatizer()\n",
    "stemmer=SnowballStemmer(\"english\")\n",
    "\n",
    "lemma_stem = []\n",
    "lemst_tokens=[]\n",
    "temp=0\n",
    "count=0\n",
    "\n",
    "i=0\n",
    "while i < (len(noun_tokens)):\n",
    "    for j in noun_tokens[i]:\n",
    "        lemma_stem.append(stemmer.stem(lemmatizer.lemmatize(j,pos=\"n\")))\n",
    "        count+=1\n",
    "        \n",
    "    lemst_tokens.append(lemma_stem[temp:count])\n",
    "    temp=count\n",
    "    i+=1\n",
    "\n",
    "\n",
    "print(len(lemma_stem))\n",
    "print(lemma_stem[0:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14675\n",
      "[['updat', 'improv'], ['worst', 'mobil', 'i', 'batteri', 'hell', 'backup', 'hour', 'us', 'idl', 'discharged.thi', 'lie', 'amazon', 'lenov', 'batteri', 'charger', 'hour', 'lenovo', 'don'], ['%', 'cash', 'januari', '..'], []]\n"
     ]
    }
   ],
   "source": [
    "print(len(lemst_tokens))\n",
    "print(lemst_tokens[0:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = [j for j in lemst_tokens[0] if not j in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['updat', 'improv']"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "\n",
    "filtered_tokens=[]\n",
    "\n",
    "result=[]\n",
    "for i in lemst_tokens:\n",
    "    output = [j for j in i if not j in stop_words]\n",
    "    filtered_tokens.append(output)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['updat', 'improv'], ['worst', 'mobil', 'batteri', 'hell', 'backup', 'hour', 'us', 'idl', 'discharged.thi', 'lie', 'amazon', 'lenov', 'batteri', 'charger', 'hour', 'lenovo'], ['%', 'cash', 'januari', '..'], []]\n"
     ]
    }
   ],
   "source": [
    "print(filtered_tokens[0:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14675"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(filtered_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "dictionary = gensim.corpora.Dictionary(filtered_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 improv\n",
      "1 updat\n",
      "2 amazon\n",
      "3 backup\n",
      "4 batteri\n",
      "5 charger\n",
      "6 discharged.thi\n",
      "7 hell\n",
      "8 hour\n",
      "9 idl\n",
      "10 lenov\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for k, v in dictionary.iteritems():\n",
    "    print(k,v)\n",
    "    count +=1\n",
    "    if count>10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary.filter_extremes(no_below=15, no_above=0.1, keep_n=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_corpus = [dictionary.doc2bow(doc) for doc in filtered_tokens]\n",
    "\n",
    "document_num = 20\n",
    "bow_doc_x = bow_corpus[document_num]\n",
    "\n",
    "for i in range(len(bow_doc_x)):\n",
    "    print(\"Word {} (\\\"{}\\\") appears {} time.\".format(bow_doc_x[i][0],\n",
    "                                                    dictionary[bow_doc_x[i][0]],\n",
    "                                                    bow_doc_x[i][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = gensim.models.LdaModel(bow_corpus,\n",
    "                                  num_topics=12,\n",
    "                                  id2word=dictionary,\n",
    "                                  passes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model1 = gensim.models.LdaMulticore(bow_corpus,\n",
    "                                  num_topics=12,\n",
    "                                  id2word=dictionary,\n",
    "                                  passes=10,\n",
    "                                  workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model2 = gensim.models.LdaMulticore(bow_corpus,\n",
    "                                  num_topics=12,\n",
    "                                  id2word=dictionary,\n",
    "                                  passes=10,\n",
    "                                  workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.272*\"problem\" + 0.160*\"perform\" + 0.064*\"heat\" + 0.056*\"month\" + 0.052*\"life\" + 0.037*\"hang\" + 0.035*\"h\" + 0.024*\"mark\" + 0.021*\"thing\" + 0.021*\"ok\"\n",
      "\n",
      "\n",
      "Topic: 1 \n",
      "Words: 0.047*\"softwar\" + 0.043*\"deliveri\" + 0.041*\"android\" + 0.041*\"music\" + 0.039*\"processor\" + 0.033*\"photo\" + 0.030*\"bit\" + 0.028*\"stock\" + 0.026*\"function\" + 0.024*\"power\"\n",
      "\n",
      "\n",
      "Topic: 2 \n",
      "Words: 0.239*\"price\" + 0.169*\"....\" + 0.094*\"rang\" + 0.054*\"superb\" + 0.046*\"dolbi\" + 0.045*\"great\" + 0.042*\"thank\" + 0.028*\"atmo\" + 0.021*\"perform\" + 0.019*\"work\"\n",
      "\n",
      "\n",
      "Topic: 3 \n",
      "Words: 0.278*\"qualiti\" + 0.073*\"speaker\" + 0.056*\"sound\" + 0.035*\"pictur\" + 0.035*\"front\" + 0.030*\"budget\" + 0.029*\"smartphon\" + 0.025*\"light\" + 0.025*\"imag\" + 0.023*\"video\"\n",
      "\n",
      "\n",
      "Topic: 4 \n",
      "Words: 0.264*\"..\" + 0.178*\"issu\" + 0.139*\"nice\" + 0.097*\"heat\" + 0.057*\"network\" + 0.050*\"wast\" + 0.035*\"money\" + 0.022*\"poor\" + 0.012*\"worst\" + 0.009*\"connect\"\n",
      "\n",
      "\n",
      "Topic: 5 \n",
      "Words: 0.274*\"featur\" + 0.065*\"app\" + 0.062*\"buy\" + 0.048*\"best\" + 0.033*\"mi\" + 0.030*\"volt\" + 0.028*\"cost\" + 0.024*\"pic\" + 0.021*\"life\" + 0.021*\"lte\"\n",
      "\n",
      "\n",
      "Topic: 6 \n",
      "Words: 0.094*\"worst\" + 0.071*\"hai\" + 0.053*\"experi\" + 0.052*\"bad\" + 0.045*\"set\" + 0.036*\"notif\" + 0.031*\"ho\" + 0.027*\"plz\" + 0.025*\"k\" + 0.023*\"mah\"\n",
      "\n",
      "\n",
      "Topic: 7 \n",
      "Words: 0.098*\"time\" + 0.096*\"backup\" + 0.084*\"day\" + 0.048*\"mode\" + 0.044*\"game\" + 0.043*\"drain\" + 0.033*\"use\" + 0.033*\"return\" + 0.032*\"depth\" + 0.030*\"amazon\"\n",
      "\n",
      "\n",
      "Topic: 8 \n",
      "Words: 0.103*\"screen\" + 0.075*\"money\" + 0.042*\"valu\" + 0.040*\"ram\" + 0.036*\"devic\" + 0.034*\"updat\" + 0.034*\"glass\" + 0.031*\"cast\" + 0.022*\"gb\" + 0.020*\"core\"\n",
      "\n",
      "\n",
      "Topic: 9 \n",
      "Words: 0.109*\"call\" + 0.061*\".....\" + 0.055*\"handset\" + 0.038*\"record\" + 0.038*\"display\" + 0.038*\"super\" + 0.034*\"expect\" + 0.034*\"*\" + 0.031*\"good\" + 0.027*\"option\"\n",
      "\n",
      "\n",
      "Topic: 10 \n",
      "Words: 0.151*\"lenovo\" + 0.102*\"note\" + 0.067*\"k8\" + 0.047*\"servic\" + 0.046*\"amazon\" + 0.038*\"issu\" + 0.023*\"sim\" + 0.021*\"custom\" + 0.020*\"time\" + 0.019*\"day\"\n",
      "\n",
      "\n",
      "Topic: 11 \n",
      "Words: 0.106*\"charg\" + 0.090*\"hour\" + 0.088*\"charger\" + 0.068*\"awesom\" + 0.065*\"%\" + 0.046*\"turbo\" + 0.038*\"day\" + 0.038*\"hr\" + 0.032*\"star\" + 0.029*\"everyth\"\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print(\"Topic: {} \\nWords: {}\".format(idx, topic))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Coherence Score: 0.47453224618476075\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import CoherenceModel\n",
    "coherence_model_lda = CoherenceModel(model=lda_model,texts=filtered_tokens,dictionary=dictionary,coherence='c_v')\n",
    "\n",
    "coherence_lda=coherence_model_lda.get_coherence()\n",
    "\n",
    "print('\\nCoherence Score:', coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Really killer experience...', 'Heating a lot and want to return', 'Nice phone', 'Awesome product.a) good battery backupb) optimised performance along with 4 GB ram and octacore processorc) Dolby Atmos speakerd) overall a worthy phonee) awesome cameraHighly recommended to all who are comparing with xi red mi note 5 pro', 'Vary Nice product']\n",
      "2935\n"
     ]
    }
   ],
   "source": [
    "\n",
    "unseen_document = list(ds_test.iloc[:,1])\n",
    "print(unseen_document[0:5])\n",
    "print(len(unseen_document))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    #Punctuation removal\n",
    "    import re, string\n",
    "    nopunc_reviews=[]\n",
    "    for i in text:\n",
    "        result = re.sub('[%s]' % re.escape(string.punctuation),\n",
    "                        ' ', i)\n",
    "        nopunc_reviews.append(result)\n",
    "    #Tokenization\n",
    "    from nltk.tokenize import word_tokenize\n",
    "    tokens=[]\n",
    "\n",
    "    for token in nopunc_reviews:\n",
    "        tokens.append(word_tokenize(token))\n",
    "        \n",
    "    #POS tagging\n",
    "    pos_tokens=[]\n",
    "\n",
    "    for tag in tokens:\n",
    "        pos_tokens.append(nltk.pos_tag(tag))\n",
    "        \n",
    "    # Topic Modeling\n",
    "      ##Extracting Nouns\n",
    "        nouns =[]\n",
    "        noun_tokens=[]\n",
    "        index=[]\n",
    "        temp =0\n",
    "        count=0\n",
    "        i=0\n",
    "    while i<len(pos_tokens):\n",
    "        for word,pos in pos_tokens[i]:\n",
    "            if (pos == 'NN' or pos == 'NNP' or \n",
    "                pos == 'NNS' or pos == 'NNPS'):\n",
    "                nouns.append(word)\n",
    "                count+=1\n",
    "            \n",
    "        noun_tokens.append(nouns[temp:count])\n",
    "        temp=count\n",
    "        \n",
    "        i+=1\n",
    "        \n",
    "        ##Lemmatize and stemming\n",
    "    from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "    lemmatizer=WordNetLemmatizer()\n",
    "    stemmer=SnowballStemmer(\"english\")\n",
    "\n",
    "    lemma_stem = []\n",
    "    lemst_tokens=[]\n",
    "    temp=0\n",
    "    count=0\n",
    "\n",
    "    i=0\n",
    "    while i < (len(noun_tokens)):\n",
    "        for j in noun_tokens[i]:\n",
    "            lemma_stem.append(stemmer.stem(\n",
    "                lemmatizer.lemmatize(j,pos=\"n\")))\n",
    "            count+=1\n",
    "            \n",
    "        lemst_tokens.append(lemma_stem[temp:count])\n",
    "        temp=count\n",
    "        \n",
    "        i+=1\n",
    "        \n",
    "        \n",
    "        ##Stopwords\n",
    "        \n",
    "    from nltk.corpus import stopwords\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "\n",
    "    filtered_tokens=[]\n",
    "\n",
    "    result=[]\n",
    "    for i in lemst_tokens:\n",
    "        output = [j for j in i if not j in stop_words]\n",
    "        filtered_tokens.append(output)\n",
    "        \n",
    "        \n",
    "    return filtered_tokens\n",
    "    \n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "decoding to str: need a bytes-like object, list found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-163-92f48849a229>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mbow_vector\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdoc2bow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0munseen_document\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\gensim\\corpora\\dictionary.py\u001b[0m in \u001b[0;36mdoc2bow\u001b[1;34m(self, document, allow_update, return_missing)\u001b[0m\n\u001b[0;32m    255\u001b[0m         \u001b[0mcounter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    256\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdocument\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 257\u001b[1;33m             \u001b[0mcounter\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mw\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0municode\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0municode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    258\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    259\u001b[0m         \u001b[0mtoken2id\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoken2id\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: decoding to str: need a bytes-like object, list found"
     ]
    }
   ],
   "source": [
    "bow_vector = dictionary.doc2bow(preprocess(unseen_document))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    #Punctuation removal\n",
    "    import re, string\n",
    "    nopunc_reviews=[]\n",
    "    for i in reviews:\n",
    "        result = re.sub('[%s]' % re.escape(string.punctuation),\n",
    "                        ' ', i)\n",
    "        nopunc_reviews.append(result)\n",
    "    #Tokenization\n",
    "    from nltk.tokenize import word_tokenize\n",
    "    tokens=[]\n",
    "\n",
    "    for token in nopunc_reviews:\n",
    "        tokens.append(word_tokenize(token))\n",
    "        \n",
    "    #POS tagging\n",
    "    pos_tokens=[]\n",
    "\n",
    "    for tag in tokens:\n",
    "        pos_tokens.append(nltk.pos_tag(tag))\n",
    "        \n",
    "    # Topic Modeling\n",
    "      ##Extracting Nouns\n",
    "        nouns =[]\n",
    "        noun_tokens=[]\n",
    "        index=[]\n",
    "        temp =0\n",
    "        count=0\n",
    "        i=0\n",
    "    while i<len(pos_tokens):\n",
    "        for word,pos in pos_tokens[i]:\n",
    "            if (pos == 'NN' or pos == 'NNP' or \n",
    "                pos == 'NNS' or pos == 'NNPS'):\n",
    "                nouns.append(word)\n",
    "                count+=1\n",
    "            \n",
    "        noun_tokens.append(nouns[temp:count])\n",
    "        temp=count\n",
    "        \n",
    "        i+=1\n",
    "        \n",
    "        ##Lemmatize and stemming\n",
    "    from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "    lemmatizer=WordNetLemmatizer()\n",
    "    stemmer=SnowballStemmer(\"english\")\n",
    "\n",
    "    lemma_stem = []\n",
    "    lemst_tokens=[]\n",
    "    temp=0\n",
    "    count=0\n",
    "\n",
    "    i=0\n",
    "    while i < (len(noun_tokens)):\n",
    "        for j in noun_tokens[i]:\n",
    "            lemma_stem.append(stemmer.stem(\n",
    "                lemmatizer.lemmatize(j,pos=\"n\")))\n",
    "            count+=1\n",
    "            \n",
    "        lemst_tokens.append(lemma_stem[temp:count])\n",
    "        temp=count\n",
    "        \n",
    "        i+=1\n",
    "        \n",
    "        \n",
    "        ##Stopwords\n",
    "        \n",
    "    from nltk.corpus import stopwords\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "\n",
    "    filtered_tokens=[]\n",
    "\n",
    "    result=[]\n",
    "    for i in lemst_tokens:\n",
    "        output = [j for j in i if not j in stop_words]\n",
    "        filtered_tokens.append(output)\n",
    "        \n",
    "        \n",
    "    return filtered_tokens\n",
    "    \n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, score in sorted(lda_model[bow_vector], key=lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
